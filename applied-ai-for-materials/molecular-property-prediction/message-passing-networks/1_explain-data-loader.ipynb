{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a TF Data Loader\n",
    "This notebook shows how we go from serialized TFRecords, which we saved in the [previous notebook](./0_save-training-data.ipynb) to training batches. TF Data Loaders are created by defining a set of operations that convert a stream of data using [Tensorflow's Data module](https://www.tensorflow.org/guide/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'datasets/train_data.proto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Records from Disk\n",
    "Our records are stored as serialized protobuf records. Origin of the stream of data will be to read from this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = tf.data.TFRecordDataset(data_file)\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this loader produces a dataset of strings, which are the serialized data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\n\\x89\\x02\\n\\x0f\\n\\x06n_atom\\x12\\x05\\x1a\\x03\\n\\x01\\x13\\n4\\n\\x04bond\\x12,\\x1a*\\n(\\x02\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x0f\\n\\x06n_bond\\x12\\x05\\x1a\\x03\\n\\x01(\\n\\x13\\n\\x07u0_atom\\x12\\x08\\x12\\x06\\n\\x04\\xbb%@\\xc0\\nd\\n\\x0cconnectivity\\x12T\\x1aR\\nP\\x00\\x01\\x00\\t\\x01\\x00\\x01\\x02\\x02\\x01\\x02\\x03\\x02\\n\\x02\\x0b\\x03\\x02\\x03\\x04\\x03\\x05\\x03\\x08\\x04\\x03\\x04\\x05\\x04\\x0c\\x04\\r\\x05\\x03\\x05\\x04\\x05\\x06\\x05\\x0e\\x06\\x05\\x06\\x07\\x07\\x06\\x07\\x08\\x07\\x0f\\x07\\x10\\x08\\x03\\x08\\x07\\x08\\x11\\x08\\x12\\t\\x00\\n\\x02\\x0b\\x02\\x0c\\x04\\r\\x04\\x0e\\x05\\x0f\\x07\\x10\\x07\\x11\\x08\\x12\\x08\\n\\x1f\\n\\x04atom\\x12\\x17\\x1a\\x15\\n\\x13\\x01\\x01\\x01\\x01\\x01\\x01\\x03\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x13\\n\\x07bandgap\\x12\\x08\\x12\\x06\\n\\x04\\xe9\\xb7\\x8f>'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance reasons, we are going to form these records into batches first before parsing.\n",
    "\n",
    "- `shuffle` creates a buffer of entries (128 in our case) and randomly picks an entry from that buffer\n",
    "- `batch` pulls multiple entries and returns them as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None,), types: tf.string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = tf.data.TFRecordDataset(data_file).shuffle(128).batch(2)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'\\n\\x84\\x02\\n2\\n\\x04bond\\x12*\\x1a(\\n&\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x0f\\n\\x06n_atom\\x12\\x05\\x1a\\x03\\n\\x01\\x14\\n\\x13\\n\\x07bandgap\\x12\\x08\\x12\\x06\\n\\x042U\\x90>\\n \\n\\x04atom\\x12\\x18\\x1a\\x16\\n\\x14\\x01\\x01\\x01\\x01\\x01\\x03\\x03\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x13\\n\\x07u0_atom\\x12\\x08\\x12\\x06\\n\\x04\\x06H>\\xc0\\n`\\n\\x0cconnectivity\\x12P\\x1aN\\nL\\x00\\x01\\x00\\x08\\x00\\t\\x00\\n\\x01\\x00\\x01\\x02\\x01\\x0b\\x01\\x0c\\x02\\x01\\x02\\x03\\x02\\x04\\x02\\r\\x03\\x02\\x03\\x0e\\x03\\x0f\\x03\\x10\\x04\\x02\\x04\\x05\\x04\\x06\\x05\\x04\\x06\\x04\\x06\\x07\\x07\\x06\\x07\\x11\\x07\\x12\\x07\\x13\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x01\\x0c\\x01\\r\\x02\\x0e\\x03\\x0f\\x03\\x10\\x03\\x11\\x07\\x12\\x07\\x13\\x07\\n\\x0f\\n\\x06n_bond\\x12\\x05\\x1a\\x03\\n\\x01&',\n",
       "       b'\\n\\xf4\\x01\\nX\\n\\x0cconnectivity\\x12H\\x1aF\\nD\\x00\\x01\\x00\\t\\x01\\x00\\x01\\x02\\x02\\x01\\x02\\x03\\x02\\x04\\x02\\x06\\x03\\x02\\x03\\x04\\x03\\n\\x03\\x0b\\x04\\x02\\x04\\x03\\x04\\x05\\x04\\x0c\\x05\\x04\\x05\\x06\\x05\\x07\\x05\\r\\x06\\x02\\x06\\x05\\x06\\x0e\\x06\\x0f\\x07\\x05\\x07\\x08\\x08\\x07\\t\\x00\\n\\x03\\x0b\\x03\\x0c\\x04\\r\\x05\\x0e\\x06\\x0f\\x06\\n\\x13\\n\\x07bandgap\\x12\\x08\\x12\\x06\\n\\x04B\\xcf\\x86>\\n\\x1c\\n\\x04atom\\x12\\x14\\x1a\\x12\\n\\x10\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x0f\\n\\x06n_bond\\x12\\x05\\x1a\\x03\\n\\x01\"\\n\\x0f\\n\\x06n_atom\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x13\\n\\x07u0_atom\\x12\\x08\\x12\\x06\\n\\x04\\xad0-\\xc0\\n.\\n\\x04bond\\x12&\\x1a$\\n\"\\x02\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a two member batch of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to convert the batches of records into a set of tensors.\n",
    "\n",
    "To do so, you must define which elements of the protobuf message you would like to read from the object and define their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_records(example_proto):\n",
    "    \"\"\"Parse data from the TFRecord\"\"\"\n",
    "    features = {\n",
    "        'u0_atom': tf.io.FixedLenFeature([], tf.float32, default_value=np.nan),\n",
    "        'n_atom': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'n_bond': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'connectivity': tf.io.VarLenFeature(tf.int64),\n",
    "        'atom': tf.io.VarLenFeature(tf.int64),\n",
    "        'bond': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_example(example_proto, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this function to the data chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {atom: (None, None), bond: (None, None), connectivity: (None, None), n_atom: (None,), n_bond: (None,), u0_atom: (None,)}, types: {atom: tf.int64, bond: tf.int64, connectivity: tf.int64, n_atom: tf.int64, n_bond: tf.int64, u0_atom: tf.float32}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = tf.data.TFRecordDataset(data_file).shuffle(128).batch(2).map(parse_records)\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have Tensor objects in a dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing to Make MPNN-compatible batches\n",
    "Tensorflow now can understand the data types, but these data are not yet in a form we can use in our MPNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fc0d06e5cd0>,\n",
       " 'bond': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fc0d06e5450>,\n",
       " 'connectivity': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fc0d0639690>,\n",
       " 'n_atom': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([14, 13])>,\n",
       " 'n_bond': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([28, 26])>,\n",
       " 'u0_atom': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-2.301325, -2.002793], dtype=float32)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our big issue is that the `SparseTensor` objects cannot be used in many Tensorflow operations. We need to convert them to Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_batching(dataset):\n",
    "    \"\"\"Make the variable length arrays into RaggedArrays.\n",
    "    \n",
    "    Allows them to be merged together in batches\"\"\"\n",
    "    for c in ['atom', 'bond', 'connectivity']:\n",
    "        expanded = tf.expand_dims(dataset[c].values, axis=0, name=f'expand_{c}')\n",
    "        dataset[c] = tf.RaggedTensor.from_tensor(expanded)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {atom: (1, None), bond: (1, None), connectivity: (1, None), n_atom: (None,), n_bond: (None,), u0_atom: (None,)}, types: {atom: tf.int64, bond: tf.int64, connectivity: tf.int64, n_atom: tf.int64, n_bond: tf.int64, u0_atom: tf.float32}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = tf.data.TFRecordDataset(data_file).shuffle(128).batch(2).map(parse_records).map(prepare_for_batching)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom': <tf.RaggedTensor [[1, 1, 1, 1, 3, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0]]>,\n",
       " 'bond': <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 0, 0, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]>,\n",
       " 'connectivity': <tf.RaggedTensor [[0, 1, 0, 9, 0, 10, 0, 11, 1, 0, 1, 2, 1, 8, 1, 12, 2, 1, 2, 3, 2, 13, 2, 14, 3, 2, 3, 4, 3, 8, 3, 15, 4, 3, 4, 5, 5, 4, 5, 6, 5, 16, 5, 17, 6, 5, 6, 7, 6, 8, 7, 6, 8, 1, 8, 3, 8, 6, 8, 18, 9, 0, 10, 0, 11, 0, 12, 1, 13, 2, 14, 2, 15, 3, 16, 5, 17, 5, 18, 8, 0, 1, 0, 9, 0, 10, 1, 0, 1, 2, 1, 6, 2, 1, 2, 3, 3, 2, 3, 4, 3, 5, 4, 3, 4, 11, 5, 3, 5, 6, 5, 7, 6, 1, 6, 5, 6, 12, 7, 5, 7, 8, 8, 7, 8, 13, 9, 0, 10, 0, 11, 4, 12, 6, 13, 8]]>,\n",
       " 'n_atom': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([19, 14])>,\n",
       " 'n_bond': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([40, 28])>,\n",
       " 'u0_atom': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-2.979503, -2.329538], dtype=float32)>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data close to the form we need it, minus a few things:\n",
    "\n",
    "- We can't easily know which \"node\" corresponds to which training entry because we have stuck multiple graphs into the same batch\n",
    "- The `connectivity` array is the wrong shape. It is a 1D instead of Nx2 array\n",
    "- The node ids in the connectivity arrays are incorrect. Since we have merged multiple graphs, the node 0 of the second graph is no longer at position 0 in the `atom` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_graphs(batch):\n",
    "    \"\"\"Combine multiple graphs into a single network\"\"\"\n",
    "    \n",
    "    # Convert RaggedTensors into vectors\n",
    "    for c in ['atom', 'bond', 'connectivity']:\n",
    "        expanded = tf.expand_dims(batch[c].values, axis=0, name=f'expand_{c}')\n",
    "        batch[c] = tf.RaggedTensor.from_tensor(expanded).flat_values\n",
    "\n",
    "    # Compute the mappings from bond index to graph index\n",
    "    batch_size = tf.size(batch['n_atom'], name='batch_size')\n",
    "    mol_id = tf.range(batch_size, name='mol_inds')\n",
    "    batch['node_graph_indices'] = tf.repeat(mol_id, batch['n_atom'], axis=0)\n",
    "    batch['bond_graph_indices'] = tf.repeat(mol_id, batch['n_bond'], axis=0)\n",
    "\n",
    "    # Reshape the connectivity matrix to (None, 2)\n",
    "    batch['connectivity'] = tf.reshape(batch['connectivity'], (-1, 2))\n",
    "\n",
    "    # Compute offsets for the connectivity matrix\n",
    "    offset_values = tf.cumsum(batch['n_atom'], exclusive=True)\n",
    "    offsets = tf.repeat(offset_values, batch['n_bond'], name='offsets', axis=0)\n",
    "    batch['connectivity'] += tf.expand_dims(offsets, 1)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {atom: (None,), bond: (None,), connectivity: (None, 2), n_atom: (None,), n_bond: (None,), u0_atom: (None,), node_graph_indices: (None,), bond_graph_indices: (None,)}, types: {atom: tf.int64, bond: tf.int64, connectivity: tf.int64, n_atom: tf.int64, n_bond: tf.int64, u0_atom: tf.float32, node_graph_indices: tf.int32, bond_graph_indices: tf.int32}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = tf.data.TFRecordDataset(data_file).shuffle(128).batch(2).map(parse_records).map(prepare_for_batching).map(combine_graphs)\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely at the output, you will notice two new values:\n",
    "- `node_graph_indices`, which stores which molecule (\"graph\") each atom (\"node\") belongs to.\n",
    "- `bond_graph_indices`, which stores which molecule each bond belongs to.\n",
    "\n",
    "This operation converts our vectors into the right shape now. And, because we did all of these operations in Tensorflow, we can delegate these operations to the GPU and use Tensorflow's automated parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom': <tf.Tensor: shape=(38,), dtype=int64, numpy=\n",
       " array([1, 1, 1, 1, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 3, 1, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])>,\n",
       " 'bond': <tf.Tensor: shape=(78,), dtype=int64, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>,\n",
       " 'connectivity': <tf.Tensor: shape=(78, 2), dtype=int64, numpy=\n",
       " array([[ 0,  1],\n",
       "        [ 0,  9],\n",
       "        [ 0, 10],\n",
       "        [ 0, 11],\n",
       "        [ 1,  0],\n",
       "        [ 1,  2],\n",
       "        [ 1,  8],\n",
       "        [ 1, 12],\n",
       "        [ 2,  1],\n",
       "        [ 2,  3],\n",
       "        [ 2, 13],\n",
       "        [ 2, 14],\n",
       "        [ 3,  2],\n",
       "        [ 3,  4],\n",
       "        [ 3, 15],\n",
       "        [ 3, 16],\n",
       "        [ 4,  3],\n",
       "        [ 4,  5],\n",
       "        [ 4,  8],\n",
       "        [ 5,  4],\n",
       "        [ 5,  6],\n",
       "        [ 5, 17],\n",
       "        [ 6,  5],\n",
       "        [ 6,  7],\n",
       "        [ 6, 18],\n",
       "        [ 7,  6],\n",
       "        [ 8,  1],\n",
       "        [ 8,  4],\n",
       "        [ 8, 19],\n",
       "        [ 9,  0],\n",
       "        [10,  0],\n",
       "        [11,  0],\n",
       "        [12,  1],\n",
       "        [13,  2],\n",
       "        [14,  2],\n",
       "        [15,  3],\n",
       "        [16,  3],\n",
       "        [17,  5],\n",
       "        [18,  6],\n",
       "        [19,  8],\n",
       "        [20, 21],\n",
       "        [20, 29],\n",
       "        [20, 30],\n",
       "        [20, 31],\n",
       "        [21, 20],\n",
       "        [21, 22],\n",
       "        [21, 28],\n",
       "        [21, 32],\n",
       "        [22, 21],\n",
       "        [22, 23],\n",
       "        [22, 28],\n",
       "        [22, 33],\n",
       "        [23, 22],\n",
       "        [23, 24],\n",
       "        [23, 34],\n",
       "        [23, 35],\n",
       "        [24, 23],\n",
       "        [24, 25],\n",
       "        [25, 24],\n",
       "        [25, 26],\n",
       "        [25, 36],\n",
       "        [25, 37],\n",
       "        [26, 25],\n",
       "        [26, 27],\n",
       "        [26, 28],\n",
       "        [27, 26],\n",
       "        [28, 21],\n",
       "        [28, 22],\n",
       "        [28, 26],\n",
       "        [29, 20],\n",
       "        [30, 20],\n",
       "        [31, 20],\n",
       "        [32, 21],\n",
       "        [33, 22],\n",
       "        [34, 23],\n",
       "        [35, 23],\n",
       "        [36, 25],\n",
       "        [37, 25]])>,\n",
       " 'n_atom': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([20, 18])>,\n",
       " 'n_bond': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([40, 38])>,\n",
       " 'u0_atom': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-3.120724, -2.787848], dtype=float32)>,\n",
       " 'node_graph_indices': <tf.Tensor: shape=(38,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>,\n",
       " 'bond_graph_indices': <tf.Tensor: shape=(78,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
